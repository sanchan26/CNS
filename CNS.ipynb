{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FdW7jMFuZG2g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "from torch.autograd.functional import hessian\n",
        "from torch.nn.utils import _stateless"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])"
      ],
      "metadata": {
        "id": "wDKBTYLCvXv0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = datasets.MNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\n",
        "valset = datasets.MNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "12Q61so3vYdG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print(images.shape)\n",
        "# print(labels.shape)"
      ],
      "metadata": {
        "id": "cFtD4aj4vfzG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I6gOMZavqZZ",
        "outputId": "17fc80eb-9044-4592-dcfe-a5eaf08833a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (5): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss()\n",
        "images, labels = next(iter(trainloader))\n",
        "print(images.shape)\n",
        "images = images.view(images.shape[0], -1)\n",
        "print(images.shape)\n",
        "logps = model(images) \n",
        "loss = criterion(logps, labels)\n",
        "print(loss) \n",
        "print(logps.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0npkEFWvtV6",
        "outputId": "46bec8d2-515b-48d1-c719-4af87a6a8de6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64, 784])\n",
            "tensor(2.3046, grad_fn=<NllLossBackward0>)\n",
            "torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before backward pass: \\n', model[0].weight.grad)\n",
        "loss.backward()\n",
        "print('After backward pass: \\n', model[4].weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm6PLbZ_vxtD",
        "outputId": "bba68ade-f291-44c2-c517-43b8e5c040aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before backward pass: \n",
            " None\n",
            "After backward pass: \n",
            " tensor([[ 0.0000e+00,  1.5453e-03,  7.4160e-05,  1.4030e-04,  1.4771e-03,\n",
            "          5.3459e-04, -3.8966e-03,  1.4334e-02,  3.6914e-03,  1.5767e-03,\n",
            "          5.2435e-03, -2.6516e-03, -2.4836e-04,  1.0339e-03,  4.5706e-05,\n",
            "          1.2322e-03,  2.1753e-04,  3.4726e-04, -1.7591e-03,  1.1299e-02,\n",
            "         -1.0000e-02,  3.7687e-03, -5.5512e-03,  1.4427e-03,  5.1039e-03,\n",
            "         -2.3901e-03,  1.5269e-03,  8.7992e-05,  1.0616e-02,  6.3586e-03,\n",
            "          7.0162e-04, -3.7762e-04, -5.5781e-03,  2.4017e-05,  0.0000e+00,\n",
            "         -1.0679e-03,  7.9320e-05,  1.0696e-02,  1.1326e-02, -2.3999e-03,\n",
            "         -3.7648e-04, -1.1017e-02,  7.3742e-05, -9.5129e-03,  5.6308e-03,\n",
            "         -9.0303e-04,  2.6498e-03,  0.0000e+00,  7.7746e-05, -8.7291e-04,\n",
            "          1.1455e-03,  9.7154e-04,  1.8541e-03, -1.0392e-04,  0.0000e+00,\n",
            "          1.4327e-04,  1.5211e-03,  7.0983e-04,  1.7299e-04, -1.2977e-03,\n",
            "          1.4544e-04, -4.2536e-03, -9.4178e-04,  2.5260e-03],\n",
            "        [ 0.0000e+00, -4.4268e-03,  7.1904e-05,  1.2150e-04, -1.3813e-03,\n",
            "         -1.0801e-02, -8.3905e-03, -1.4568e-02, -8.4891e-03, -3.2502e-03,\n",
            "         -7.5345e-03,  2.5146e-04, -1.5305e-02,  1.7858e-03, -1.8704e-03,\n",
            "         -2.5977e-04,  1.9818e-04, -6.6872e-04,  1.1287e-03, -5.0215e-03,\n",
            "         -3.3879e-03, -6.9883e-03,  3.7469e-03,  3.2062e-03, -2.0738e-02,\n",
            "          2.9595e-04, -2.3357e-03, -3.0695e-04, -9.0347e-03,  3.3984e-05,\n",
            "         -1.0456e-02, -1.7421e-02,  1.3156e-03,  2.0149e-05,  0.0000e+00,\n",
            "         -2.7791e-03,  7.2071e-04, -2.0162e-02, -1.8203e-02,  4.4165e-03,\n",
            "         -6.0021e-03,  8.0983e-03, -2.2948e-04, -6.0768e-03, -6.8320e-03,\n",
            "          3.7586e-03, -4.8288e-03,  0.0000e+00,  7.0087e-05, -5.5420e-03,\n",
            "         -3.0183e-02,  8.3128e-04, -2.1467e-03, -2.6442e-03,  0.0000e+00,\n",
            "          3.2275e-03,  9.0736e-04, -2.0623e-03,  1.9533e-04, -1.1253e-02,\n",
            "          1.2191e-04, -2.5622e-02, -1.5005e-02, -3.0145e-02],\n",
            "        [ 0.0000e+00,  1.8346e-03,  6.4132e-05,  1.1909e-04, -6.1051e-05,\n",
            "          1.0592e-02, -4.2838e-03, -1.7248e-03, -4.3629e-03,  2.2400e-03,\n",
            "          5.3759e-03,  2.6530e-04,  8.5027e-03,  4.6370e-04,  4.3752e-04,\n",
            "          1.1738e-03, -1.8880e-03,  1.2177e-03,  1.1231e-03,  6.0015e-03,\n",
            "         -6.9719e-04,  1.7891e-03,  1.4269e-03,  3.7275e-03,  5.3014e-03,\n",
            "          2.7508e-04,  1.3057e-03,  7.8371e-05,  9.5499e-03,  1.4556e-02,\n",
            "          9.2053e-03,  8.1947e-04,  1.2614e-03,  2.1403e-05,  0.0000e+00,\n",
            "          2.1111e-03, -2.3741e-03,  6.5526e-03,  7.5072e-03,  5.7314e-04,\n",
            "          7.9550e-03,  4.5810e-03, -3.3093e-04, -6.1132e-03,  4.8223e-03,\n",
            "          3.6848e-03,  3.2293e-03,  0.0000e+00,  6.5115e-05,  2.2434e-04,\n",
            "          1.2172e-02, -1.5888e-03, -5.3712e-03, -8.8450e-04,  0.0000e+00,\n",
            "          2.5676e-03,  6.6462e-04,  6.2912e-04,  3.4427e-04, -6.3995e-04,\n",
            "          1.2795e-04,  6.5560e-04,  3.9982e-03,  2.0185e-02],\n",
            "        [ 0.0000e+00,  5.4944e-04,  8.4954e-05,  1.5587e-04, -2.5721e-03,\n",
            "          6.3153e-03, -3.6819e-03,  4.3025e-03, -9.2041e-04, -3.2316e-04,\n",
            "          5.6779e-04,  3.3303e-04, -2.1313e-03,  2.1723e-03,  5.6451e-04,\n",
            "          2.2519e-03,  2.1970e-04,  1.5227e-03,  7.2707e-04,  5.0153e-03,\n",
            "          1.2743e-03,  1.0376e-03,  5.6184e-03, -3.2706e-03,  3.4862e-03,\n",
            "         -3.1471e-05,  3.6495e-04,  9.5643e-05, -1.2311e-04, -2.7336e-03,\n",
            "          7.4302e-04, -1.3034e-03, -3.9057e-03,  2.3356e-05,  0.0000e+00,\n",
            "          1.0431e-03, -1.1117e-03, -1.0909e-02,  1.2287e-02,  1.5193e-04,\n",
            "         -3.6734e-03,  1.9854e-03,  7.6240e-05, -8.8289e-04,  3.3794e-03,\n",
            "         -5.4948e-04, -5.1854e-03,  0.0000e+00, -6.5693e-04,  4.1579e-04,\n",
            "          1.0929e-02,  1.0043e-03,  1.0099e-03,  1.6765e-03,  0.0000e+00,\n",
            "          3.6364e-03, -4.8010e-03,  7.7675e-04,  4.2736e-04,  9.3060e-03,\n",
            "          1.5148e-04,  8.2445e-03, -8.3051e-04,  5.6594e-03],\n",
            "        [ 0.0000e+00,  2.0847e-03,  7.4410e-05, -1.9403e-04,  1.0670e-03,\n",
            "          9.6145e-03,  6.5702e-03,  1.3146e-02,  7.0261e-03,  2.9266e-03,\n",
            "          4.9126e-03,  3.0445e-04,  8.7307e-03, -2.0505e-03,  4.9647e-04,\n",
            "          2.1343e-04,  2.0503e-04, -1.8718e-03,  2.9346e-04,  3.4166e-04,\n",
            "          4.2662e-03,  2.4219e-03, -8.2310e-04, -4.9451e-04,  7.8745e-03,\n",
            "          3.0565e-04, -1.8025e-04,  8.9763e-05,  1.2372e-02,  8.3013e-03,\n",
            "          1.0067e-02,  1.1952e-02,  1.2534e-03,  2.1895e-05,  0.0000e+00,\n",
            "          1.4255e-03,  7.5156e-04,  2.0273e-02,  1.0238e-02, -3.0243e-03,\n",
            "          3.8641e-03,  7.4667e-03,  6.7551e-05,  1.2397e-02,  7.1121e-03,\n",
            "         -6.1341e-04,  2.0815e-03,  0.0000e+00,  7.7416e-05,  8.8381e-04,\n",
            "          2.2726e-02, -3.1381e-04,  3.0438e-03,  3.5582e-03,  0.0000e+00,\n",
            "          2.8464e-04, -2.3867e-04,  7.2403e-04,  3.9187e-04,  1.9722e-02,\n",
            "         -1.1849e-03,  2.2924e-02,  1.2910e-02,  2.0457e-02],\n",
            "        [ 0.0000e+00,  1.5155e-03,  7.5521e-05,  5.9063e-05, -4.6604e-03,\n",
            "         -7.2799e-03,  9.1046e-03, -5.1507e-04, -1.1728e-03, -7.5905e-04,\n",
            "          9.7737e-04,  3.2989e-04, -4.4273e-03, -1.3627e-03,  5.5731e-04,\n",
            "          1.4155e-03,  2.3486e-04,  2.4945e-04,  1.3168e-03, -1.2252e-02,\n",
            "          2.3559e-03,  1.1894e-04, -7.0559e-03, -7.2148e-03,  3.4186e-03,\n",
            "          3.3453e-04, -7.4874e-04,  8.9307e-05, -3.9430e-03, -1.3570e-02,\n",
            "          5.8967e-04, -7.0578e-04, -1.8785e-04,  2.3891e-05,  0.0000e+00,\n",
            "          1.7395e-03,  8.1709e-04, -7.9655e-03, -1.0163e-02,  4.3447e-03,\n",
            "          6.3363e-04,  3.3540e-03,  7.6571e-05,  5.8031e-03,  3.8255e-03,\n",
            "         -1.2248e-03,  6.9526e-04,  0.0000e+00,  8.0292e-05,  1.6068e-04,\n",
            "          1.1883e-03, -8.3762e-04, -5.2959e-04, -1.8636e-03,  0.0000e+00,\n",
            "          8.3045e-04,  2.1691e-04, -2.8430e-04,  3.9543e-04, -7.8248e-04,\n",
            "          1.3291e-04,  9.6906e-04,  1.2887e-04, -9.5367e-03],\n",
            "        [ 0.0000e+00, -7.7339e-03,  6.3857e-05, -1.3302e-06,  4.5482e-03,\n",
            "          2.2325e-03,  5.0310e-04, -9.1057e-03, -4.9066e-03,  1.7956e-04,\n",
            "          2.5481e-03,  2.6154e-04, -7.7474e-03, -1.3804e-03,  4.4107e-04,\n",
            "         -2.8472e-03,  1.8796e-04, -1.7919e-03,  1.0345e-03,  9.3340e-04,\n",
            "          3.6125e-03, -4.0747e-04,  4.4113e-03,  2.9140e-03, -3.3903e-04,\n",
            "          2.7516e-04, -5.8891e-04,  7.7975e-05, -1.9982e-02, -4.2354e-03,\n",
            "         -5.1644e-03,  2.6932e-04, -4.3283e-03,  2.0813e-05,  0.0000e+00,\n",
            "          2.4026e-03,  6.9924e-04, -9.4080e-03,  6.6219e-05,  3.6324e-03,\n",
            "          3.5096e-03,  5.9713e-03,  6.2486e-05,  3.2439e-03, -1.1986e-02,\n",
            "         -3.3536e-03, -1.0829e-03,  0.0000e+00,  6.6624e-05,  2.2881e-03,\n",
            "         -1.8890e-02,  1.3937e-04, -5.1932e-03, -3.1693e-03,  0.0000e+00,\n",
            "          3.2014e-03,  1.4994e-03,  5.2505e-04, -1.6969e-03, -1.5116e-02,\n",
            "          1.1903e-04, -3.6976e-03,  5.1312e-03,  7.1398e-03],\n",
            "        [ 0.0000e+00,  1.9592e-03, -4.2175e-04,  1.2463e-04,  1.8350e-03,\n",
            "         -2.7448e-03,  1.1790e-02,  7.2865e-03,  5.9454e-03,  2.7108e-03,\n",
            "         -4.0996e-03,  2.8018e-04,  7.3679e-03,  1.3860e-03,  7.8552e-05,\n",
            "         -2.0895e-03,  1.9708e-04, -1.5894e-03, -2.4256e-03,  2.3029e-03,\n",
            "          2.4563e-03,  3.5455e-03,  2.5311e-03,  6.6722e-05, -4.4915e-04,\n",
            "          2.8601e-04,  1.3857e-03,  8.2198e-05,  1.3976e-02, -3.5188e-04,\n",
            "          7.0722e-03,  4.5653e-03,  3.0016e-03,  2.0648e-05,  0.0000e+00,\n",
            "         -2.9575e-03,  7.2782e-04,  1.4669e-02,  7.8661e-04,  9.8754e-04,\n",
            "         -3.7259e-03,  2.9510e-03,  6.5846e-05,  7.2959e-03,  1.1236e-02,\n",
            "         -2.1560e-03, -1.4963e-04,  0.0000e+00,  7.0921e-05,  3.5498e-04,\n",
            "          1.2338e-02, -8.0511e-04,  2.9877e-03,  2.4590e-03,  0.0000e+00,\n",
            "         -7.3807e-03, -1.7946e-04,  6.6627e-04,  3.6512e-04,  1.7034e-02,\n",
            "          1.2835e-04,  1.0975e-02,  8.9320e-04,  9.1798e-03],\n",
            "        [ 0.0000e+00,  1.1544e-03,  7.2301e-05,  1.3415e-04,  2.6255e-03,\n",
            "         -3.6718e-03, -9.5111e-03, -1.1873e-02, -2.8768e-03, -3.7809e-03,\n",
            "          3.1716e-03,  2.9463e-04,  3.4839e-03,  2.5653e-04, -9.9518e-04,\n",
            "          1.2278e-03,  2.0802e-04,  1.2014e-03, -5.2249e-04,  3.4469e-03,\n",
            "         -4.4439e-03, -3.4228e-03,  3.3560e-03,  2.8438e-03,  9.1017e-04,\n",
            "          2.9800e-04, -9.5356e-04, -3.7920e-04, -5.4594e-03,  2.2193e-03,\n",
            "         -8.9387e-03,  3.0199e-03,  1.8196e-03, -2.0159e-04,  0.0000e+00,\n",
            "         -3.7131e-03, -1.1186e-03, -9.0071e-06, -6.4936e-03, -6.0798e-03,\n",
            "          4.5317e-03, -1.4045e-02,  6.9508e-05, -4.4737e-03, -1.1240e-02,\n",
            "          4.0663e-03,  1.9738e-03,  0.0000e+00,  7.3346e-05,  1.1224e-03,\n",
            "         -5.4589e-03, -3.6731e-04,  6.5035e-04, -2.3744e-04,  0.0000e+00,\n",
            "          3.3050e-03,  1.1917e-03, -2.3650e-03, -8.0679e-04, -1.6502e-02,\n",
            "          1.1858e-04, -1.3388e-02, -5.2837e-04, -8.8798e-03],\n",
            "        [ 0.0000e+00,  1.5177e-03, -1.5949e-04, -6.5924e-04, -2.8779e-03,\n",
            "         -4.7916e-03,  1.7963e-03, -1.2820e-03,  6.0657e-03, -1.5204e-03,\n",
            "         -1.1163e-02,  3.3117e-04,  1.7740e-03, -2.3046e-03,  2.4445e-04,\n",
            "         -2.3183e-03,  2.1959e-04,  1.3834e-03, -9.1646e-04, -1.2067e-02,\n",
            "          4.5641e-03, -1.8632e-03, -7.6605e-03, -3.2211e-03, -4.5690e-03,\n",
            "          3.5115e-04,  2.2379e-04,  8.4905e-05, -7.9716e-03, -1.0579e-02,\n",
            "         -3.8194e-03, -8.1753e-04,  5.3484e-03,  2.5422e-05,  0.0000e+00,\n",
            "          1.7957e-03,  8.0874e-04, -3.7377e-03, -7.3522e-03, -2.6023e-03,\n",
            "         -6.7161e-03, -9.3454e-03,  6.8472e-05, -1.6803e-03, -5.9484e-03,\n",
            "         -2.7094e-03,  6.1712e-04,  0.0000e+00,  7.5383e-05,  9.6482e-04,\n",
            "         -5.9670e-03,  9.6620e-04,  3.6950e-03,  1.2092e-03,  0.0000e+00,\n",
            "         -9.8155e-03, -7.8200e-04,  6.8053e-04,  2.1131e-04, -4.7062e-04,\n",
            "          1.3926e-04,  3.1923e-03, -5.7561e-03, -1.6587e-02]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Method 1 to calculate hessian:\n",
        "\n",
        "# def eval_hessian(loss_grad, model):\n",
        "#     cnt = 0\n",
        "#     for g in loss_grad:\n",
        "#         g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
        "#         cnt = 1\n",
        "#     l = g_vector.size(0)\n",
        "#     hessian = torch.zeros(l, l)\n",
        "#     for idx in range(l):\n",
        "#         grad2rd = torch.autograd.grad(g_vector[idx], model.parameters(), create_graph=True)\n",
        "#         cnt = 0\n",
        "#         for g in grad2rd:\n",
        "#             g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
        "#             cnt = 1\n",
        "#         hessian[idx] = g2\n",
        "#     return hessian.cpu().data.numpy()"
      ],
      "metadata": {
        "id": "x4ws2LGEjhb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce775dc9-6a96-4f50-d412-168ac410164a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "time0 = time()\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images = images.view(images.shape[0], -1)\n",
        "    \n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        #This is where the model learns by backpropagating\n",
        "        loss.backward()\n",
        "        \n",
        "        #And optimizes its weights here\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
        "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n",
        "print(model[0].weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ9_HgpUv3lT",
        "outputId": "98e5f51d-8ba3-4d1e-f112-6f5532329a7e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 0.6344314235836458\n",
            "Epoch 1 - Training loss: 0.2816658278327507\n",
            "Epoch 2 - Training loss: 0.22000452187031444\n",
            "Epoch 3 - Training loss: 0.17839818782230685\n",
            "Epoch 4 - Training loss: 0.14735293241817432\n",
            "\n",
            "Training Time (in minutes) = 1.4188337246576945\n",
            "Parameter containing:\n",
            "tensor([[-0.0285, -0.0279,  0.0091,  ...,  0.0074, -0.0261, -0.0006],\n",
            "        [-0.0293, -0.0035,  0.0196,  ...,  0.0311,  0.0285, -0.0139],\n",
            "        [ 0.0339, -0.0122,  0.0100,  ...,  0.0158, -0.0294,  0.0045],\n",
            "        ...,\n",
            "        [-0.0344,  0.0077,  0.0268,  ..., -0.0153, -0.0320, -0.0343],\n",
            "        [ 0.0209, -0.0103,  0.0107,  ...,  0.0077,  0.0239, -0.0136],\n",
            "        [ 0.0131, -0.0225,  0.0281,  ...,  0.0109, -0.0218,  0.0003]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model[0].weight.grad)\n",
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)\n",
        "def loss1(*params):\n",
        "    out: torch.Tensor = _stateless.functional_call(model, {n: p for n, p in zip(names, params)},images)\n",
        "    return out.square().sum()\n",
        "\n",
        "names = list(n for n, _ in model.named_parameters())\n",
        "print(loss1)\n",
        "print(model.parameters())\n",
        "\n",
        "print(hessian(loss1, tuple(model.parameters())))\n",
        "# loss_grad = torch.autograd.grad(loss, model.parameters(),retain_graph = True)\n",
        "# h = eval_hessian(loss_grad, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "GrvTgG2TRGgn",
        "outputId": "97e2a71a-a4c8-4be3-b433-92c7b823a876"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function loss1 at 0x7f1b14478700>\n",
            "<generator object Module.parameters at 0x7f1a557ecb30>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0424edb3000b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# loss_grad = torch.autograd.grad(loss, model.parameters(),retain_graph = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# h = eval_hessian(loss_grad, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m     res = jacobian(jac_func, inputs, create_graph=create_graph, strict=strict, vectorize=vectorize,\n\u001b[0m\u001b[1;32m    813\u001b[0m                    strategy=outer_jacobian_strategy)\n\u001b[1;32m    814\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tuple_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mjac_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                 vj = _autograd_grad((out.reshape(-1)[j],), inputs,\n\u001b[0m\u001b[1;32m    674\u001b[0m                                     retain_graph=True, create_graph=create_graph)\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         return torch.autograd.grad(new_outputs, inputs, new_grad_outputs, allow_unused=True,\n\u001b[0m\u001b[1;32m    160\u001b[0m                                    \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                                    is_grads_batched=is_grads_batched)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# images, labels = next(iter(valloader))\n",
        "\n",
        "# img = images[0].view(1, 784)\n",
        "# with torch.no_grad():\n",
        "#     logps = model(img)\n",
        "\n",
        "# ps = torch.exp(logps)\n",
        "# probab = list(ps.numpy()[0])\n",
        "# print(\"Predicted Digit =\", probab.index(max(probab)))\n",
        "# view_classify(img.view(1, 28, 28), ps)"
      ],
      "metadata": {
        "id": "Ba17khG4wDfe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correct_count, all_count = 0, 0\n",
        "# for images,labels in valloader:\n",
        "#   for i in range(len(labels)):\n",
        "#     img = images[i].view(1, 784)\n",
        "#     with torch.no_grad():\n",
        "#         logps = model(img)\n",
        "\n",
        "    \n",
        "#     ps = torch.exp(logps)\n",
        "#     probab = list(ps.numpy()[0])\n",
        "#     pred_label = probab.index(max(probab))\n",
        "#     true_label = labels.numpy()[i]\n",
        "#     if(true_label == pred_label):\n",
        "#       correct_count += 1\n",
        "#     all_count += 1\n",
        "\n",
        "# print(\"Number Of Images Tested =\", all_count)\n",
        "# print(\"\\nModel Accuracy =\", (correct_count/all_count))"
      ],
      "metadata": {
        "id": "orZEKoKSwGMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd.functional import hessian\n",
        "from torch.nn.utils import _stateless\n",
        "\n",
        "model = torch.nn.Linear(2, 2)\n",
        "inp = torch.rand(1, 2)\n",
        "\n",
        "def loss(*params):\n",
        "    out: torch.Tensor = _stateless.functional_call(model, {n: p for n, p in zip(names, params)}, inp)\n",
        "    return out.square().sum()\n",
        "print(model.parameters())\n",
        "print(loss())\n",
        "names = list(n for n, _ in model.named_parameters())\n",
        "print(hessian(loss, tuple(model.parameters())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZKxp2XK5y-B",
        "outputId": "7bef5c2e-92da-49b9-873f-2373dd08b76a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7fe4b072ef20>\n",
            "tensor(0.1943, grad_fn=<SumBackward0>)\n",
            "((tensor([[[[0.0852, 0.1074],\n",
            "          [0.0000, 0.0000]],\n",
            "\n",
            "         [[0.1074, 0.1353],\n",
            "          [0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000],\n",
            "          [0.0852, 0.1074]],\n",
            "\n",
            "         [[0.0000, 0.0000],\n",
            "          [0.1074, 0.1353]]]]), tensor([[[0.4128, 0.0000],\n",
            "         [0.5202, 0.0000]],\n",
            "\n",
            "        [[0.0000, 0.4128],\n",
            "         [0.0000, 0.5202]]])), (tensor([[[0.4128, 0.5202],\n",
            "         [0.0000, 0.0000]],\n",
            "\n",
            "        [[0.0000, 0.0000],\n",
            "         [0.4128, 0.5202]]]), tensor([[2., 0.],\n",
            "        [0., 2.]])))\n"
          ]
        }
      ]
    }
  ]
}